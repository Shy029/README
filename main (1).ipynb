{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZfhoLh0d6kw"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "import os\n",
        "\n",
        "# Set environment variables for Java and Spark (adjust paths as needed)\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "# ... rest of your code using Spark and Java libraries ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GaoOnz03rYbK"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.recommendation import ALS,ALSModel\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6RapdCgrkbI"
      },
      "outputs": [],
      "source": [
        "spark=SparkSession.builder.appName(\"HealthRecSystem\").getOrCreate() # type: ignore\n",
        "df=spark.read.csv(\"sym_dia_diff.csv\",header=True,inferSchema=True)\n",
        "sym=spark.read.csv(\"symptoms.csv\",header=True,inferSchema=True)\n",
        "dia=spark.read.csv(\"diagnosis.csv\",header=True,inferSchema=True)\n",
        "# print(\"symptoms\")\n",
        "# sym.show(4)\n",
        "# print(\"diagnosis\")\n",
        "# dia.show(4)\n",
        "# print(\"diff\")\n",
        "# diff.show(4)\n",
        "print(\"feature engineering started\")\n",
        "string_indexer=StringIndexer(inputCol=\"symptom\",outputCol=\"symptom_index\")\n",
        "model=string_indexer.setHandleInvalid(\"skip\").fit(df)\n",
        "indexed=model.transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fmrBCTgMsEHf"
      },
      "outputs": [],
      "source": [
        "# indexed.show(4)\n",
        "encoder=OneHotEncoder(dropLast=False,inputCols=[\"symptom_index\"],outputCols=[\"symptom_vec\"])\n",
        "encoded=encoder.fit(indexed).transform(indexed)\n",
        "df=encoded\n",
        "# df.show(4)\n",
        "string_indexer=StringIndexer(inputCol=\"diagnose\",outputCol=\"diagnose_index\")\n",
        "model=string_indexer.setHandleInvalid(\"skip\").fit(df)\n",
        "indexed=model.transform(df)\n",
        "# indexed.show(4)\n",
        "encoder=OneHotEncoder(dropLast=False,inputCols=[\"diagnose_index\"],outputCols=[\"diagnose_vec\"])\n",
        "encoded=encoder.fit(indexed).transform(indexed)\n",
        "df=encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qISG5hasJUm"
      },
      "outputs": [],
      "source": [
        "# df.show(4)\n",
        "print(\"feature engineering completed\")\n",
        "print(\"feature scaling\")\n",
        "cols=[\"symptom\",\"diagnose\"]\n",
        "for col in cols:\n",
        "    scaler=MinMaxScaler(inputCol=col+\"_vec\",outputCol=col+\"_vec_scaled\")\n",
        "    scaler_model=scaler.fit(df)\n",
        "    scaler_data=scaler_model.transform(df)\n",
        "    df=scaler_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-d83jY7osPSB"
      },
      "outputs": [],
      "source": [
        "# df.show(4)\n",
        "vec_assembler=VectorAssembler(inputCols=[\"symptom_vec_scaled\",\"diagnose_vec_scaled\"],outputCol=\"features\")\n",
        "df=vec_assembler.transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yi6JfqbKsVLx"
      },
      "outputs": [],
      "source": [
        "# df.show(4)\n",
        "print(\"feature scaling completed\")\n",
        "\n",
        "print(\"PCA and Kmeans started\")\n",
        "\n",
        "kmeans=KMeans(k=2,seed=1)\n",
        "model=kmeans.fit(df.select(\"features\"))\n",
        "transformed=model.transform(df)\n",
        "df=transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7hXo0pgsayX"
      },
      "outputs": [],
      "source": [
        "# df.show(4)\n",
        "\n",
        "pca=PCA(k=2,inputCol=\"features\",outputCol=\"pcaFeatures\")\n",
        "model=pca.fit(df)\n",
        "result=model.transform(df).select(\"pcaFeatures\")\n",
        "pandasDf=result.toPandas()\n",
        "dataX=[]\n",
        "dataY=[]\n",
        "for vec in pandasDf.values:\n",
        "    dataX.append(vec[0][0])\n",
        "    dataY.append(vec[0][1])\n",
        "print(\"PCA and Kmeans completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHJ_I88JskXb"
      },
      "outputs": [],
      "source": [
        "# plt.scatter(dataX,dataY)\n",
        "# plt.show()\n",
        "\n",
        "# sns.scatterplot(x=dataX,y=dataY)\n",
        "# plt.title(\"PCA features\")\n",
        "# plt.show()\n",
        "\n",
        "print(\"model training\")\n",
        "df=df.na.drop(subset=['syd','diagnose_index','wei'])\n",
        "df=df.drop(\"prediction\")\n",
        "df.show(4)\n",
        "print(type(df))\n",
        "splits=df.randomSplit([0.75,0.25],24)\n",
        "train=splits[0]\n",
        "test=splits[1]\n",
        "print(\"Training test size\",train.count())\n",
        "print(\"Test data size\",test.count())\n",
        "rec=ALS(maxIter=10,regParam=0.01,userCol=\"syd\",itemCol=\"diagnose_index\",nonnegative=True,ratingCol=\"wei\",coldStartStrategy=\"drop\")\n",
        "rec_model=rec.fit(train)\n",
        "rec_model.write().overwrite().save(\"./models/als_model\")\n",
        "print(\"model training completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4frQWtqsre2"
      },
      "outputs": [],
      "source": [
        "# print(\"model testing\")\n",
        "# rec_saved_model=ALSModel.load(\"./models/als_model\")\n",
        "# predicted_ratings=rec_saved_model.transform(test)\n",
        "# predicted_ratings.show(8)\n",
        "# evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"wei\",predictionCol=\"prediction\")\n",
        "# rmse=evaluator.evaluate(predicted_ratings)\n",
        "# print(\"RMSE\",rmse)\n",
        "# wei_max_value=df.agg({'wei':'max'}).collect()[0][0]\n",
        "# wei_min_value=df.agg({'wei':'min'}).collect()[0][0]\n",
        "\n",
        "print(\"Loading model\")\n",
        "rec_saved_model=ALSModel.load(\"./models/als_model\")\n",
        "print(\"Model loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxIwhyqFTzO9"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Nd8aJJgMSl2E"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession if not already initialized\n",
        "if 'spark' not in locals():\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"HealthRecSystem\") \\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2eSJ5bp8Jy1W"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_id(symptom,spark):\n",
        "    return int(df.filter(df[\"symptom\"]==symptom).select(\"syd\").collect()[0][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For Webserver using flask and ngrok\n",
        "!pip install flask-ngrok\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "zd4uk88k-AGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ngrok auth configuration\n",
        "!ngrok config add-authtoken 2fYUq1NGdUHn45NelNvECes3Py8_nAAq7BxhCyoZDVAvSWQ7"
      ],
      "metadata": {
        "id": "J--fUtuaB5nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import threading\n",
        "\n",
        "from flask import Flask, request, render_template\n",
        "from pyngrok import ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "port = \"5000\"\n",
        "\n",
        "# Open a ngrok tunnel to the HTTP server\n",
        "public_url = ngrok.connect(port).public_url #Don't run twice. Comment out before running!\n",
        "print(f\"* ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")\n",
        "\n",
        "# Update any base URLs to use the public ngrok URL\n",
        "app.config[\"BASE_URL\"] = public_url\n",
        "\n",
        "# Define Flask routes\n",
        "@app.route(\"/\")\n",
        "def index():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "# Define a route for the home page\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def home():\n",
        "    if request.method == 'POST':\n",
        "        final_symptoms = []\n",
        "        # Get user input for symptoms\n",
        "        symptoms = list(request.form.get('symptoms').split(\",\"))\n",
        "        print(\"Symptoms inputted:\", symptoms)\n",
        "        for i in symptoms:\n",
        "            capsymp = i.strip().capitalize()\n",
        "            if (sym.filter(sym[\"symptom\"] == capsymp).count() > 0):\n",
        "                final_symptoms.append(get_id(capsymp, spark))\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "        # Get the recommended diagnosis\n",
        "        df1a = df.filter(df[\"syd\"].isin(final_symptoms)).select('syd', 'did', 'diagnose_index', 'diagnose').orderBy(\n",
        "            'wei', ascending=False)\n",
        "        df1a.show(4)\n",
        "        recs = rec_saved_model.transform(df1a).orderBy('prediction', ascending=False)\n",
        "        print(\"Recommended diagnosis for symptoms:\", symptoms)\n",
        "        recs.show(10, False)\n",
        "        diagnosis = recs.select(\"diagnose\").first()[0]\n",
        "        print(diagnosis)\n",
        "\n",
        "        return render_template('index.html', diagnosis=diagnosis, symptoms=symptoms)\n",
        "\n",
        "    return render_template('index.html')\n",
        "\n",
        "# Start the Flask server in a new thread\n",
        "mainthread = threading.Thread(target=app.run, kwargs={\"use_reloader\": False})\n",
        "\n",
        "mainthread.start()\n",
        "#threading.Thread(target=app.run, kwargs={\"debug\": True}).start()"
      ],
      "metadata": {
        "id": "lAi39v8MJVkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mainthread.join()"
      ],
      "metadata": {
        "id": "-qSxlpVAPM3D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}